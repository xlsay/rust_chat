cpu: false
use_flash_attn: false

repo_id: 
model_name: 
model_config:

revision: "main"
quantized: true
# tokenizer_file:
# weight_files:
tokenizer_file: "../../candle_mistral/tokenizer.json"
weight_files:  "../../candle_mistral/Candle_Mistral-7B-v0.1_q6k.gguf"
# weight_files:  "../../candle_mistral/Candle_Mistral-7B-v0.1_q8k.gguf" # quantized type Q8K is not supported yet
